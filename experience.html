<!DOCTYPE html>
<!--
	Stellar by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
  <head>
    <title>Work Experience</title>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, user-scalable=no"
    />
    <link rel="stylesheet" href="assets/css/main.css" />
    <noscript
      ><link rel="stylesheet" href="assets/css/noscript.css"
    /></noscript>
  </head>
  <body class="is-preload">
    <!-- Wrapper -->
    <div id="wrapper">
      <!-- Header -->
      <header id="header">
        <h1>Work Experience</h1>
        <p>Waiting for this list to grow.</p>
      </header>

      <!-- Main -->
      <div id="main">
        <!-- Content -->
        <section id="content" class="main">
          <h2>Transparent Language - Software Quality Assurance Tester</h2>
          <!-- <span class="image right"><img src="images/TLalt.png" alt="" /></span> -->
          <p>
            I currently work as a Software Quality Assurance Tester at
            Transparent Language. With a constant stream of fresh builds, new
            bugs are always introduced to the system. In the four months I've
            worked here I've learned and utilized a variety of methods related
            to bug detection and software quality assurance. For example, I
            learned that we can detect and/or prevent a majority of bugs simply
            by creating a thorough test plan. However, it's not enough that the
            test plan covers every feature of the software in question;
            different scenarios for each feature should be considered as well.
            Let's say you're testing the end user interface:
          </p>
          <p>
            Clicking every button or link on the end user interface to check if
            every feature works may not be a thorough enough test. What if the
            feature is user specific? Then you would need to test that feature
            for all valid user types; it wouldn't be enough to test the feature
            with a specific user in this case.
          </p>
          <p>
            More often than not however a test plan will not cover everything
            that needs to be tested; there may be too many scenarios to list!
          </p>
          <p>
            As such, I've found it useful to test software with the developer
            console open. As long as the code was written with error detection
            in mind, this is a very convenient way to catch bugs that are
            occurring due to unexpected inputs.
          </p>
          <p>
            During my time here, I've tested many features more than once; I've
            seen how tedious manual testing can be. I've found nearly 145 bugs
            in approximately 4 months; that's a little over a bug a day. I've
            realized after all those bugs that if I were to continue my career
            in quality assurance, I'd much rather write scripts to automate my
            tests than manually test each feature repeatedly. Thus the job
            search begins!
          </p>
          <h2>Rapid7 - Clinic Program</h2>
          <p>
            As a Computer Science major at Harvey Mudd the cherry on top for my
            degree was the Clinic Program. Think of it like a thesis
            replacement. In groups, we would work for companies that paid some
            amount of money for our labor (sadly that money never went directly
            to the students). For my Clinic Program, I worked on the Rapid7
            team. The environment was very similar to my Software Development
            course (mentioned in my KSPC.org project). Every week we would have
            a telephone meeting with our client. We would discuss our current
            progress, how much work we expected we could put in by next week,
            and any adjustments the client would like to make.
          </p>
          <p>
            What my group and I completed for Rapid7 can be described as
            follows: we integrated Rapid7's container assessment service with
            continuous integration tools like Jenkins. In order to integrate
            their service, we created a plugin for each CI tool. The plugins
            would pass/fail a build based on various criteria. Finally, the
            plugins would generate a detailed report on why the build
            passed/failed.
          </p>
          <ul class="actions special">
            <li><a href="index.html" class="button">Home</a></li>
          </ul>
        </section>
      </div>

      <!-- Footer -->
      <footer id="footer">
        <section>
          <h2>Contact information</h2>
          <dl class="alt">
            <dt>Email</dt>
            <dd>enguyen@hmc.edu</dd>
            <dt>GitHub</dt>
            <dd>
              <ul class="icons">
                <li>
                  <a
                    href="https://github.com/ericnguyen11"
                    class="icon brands fa-github alt"
                    ><span class="label">GitHub</span></a
                  >
                </li>
              </ul>
            </dd>
          </dl>
        </section>
        <p class="copyright">
          Design: <a href="https://html5up.net">HTML5 UP</a>.
        </p>
      </footer>
    </div>

    <!-- Scripts -->
    <script src="assets/js/jquery.min.js"></script>
    <script src="assets/js/jquery.scrollex.min.js"></script>
    <script src="assets/js/jquery.scrolly.min.js"></script>
    <script src="assets/js/browser.min.js"></script>
    <script src="assets/js/breakpoints.min.js"></script>
    <script src="assets/js/util.js"></script>
    <script src="assets/js/main.js"></script>
  </body>
</html>
